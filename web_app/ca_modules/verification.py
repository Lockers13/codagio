import sys
import os
import json
import random
from django.utils.translation import gettext_lazy as _
from .output_processor import process_output

class Verifier:

    """Class for verifying output of submitted program, by checking against sample outputs

    previously generated by the relevant sample reference program during problem upload"""

    def __init__(self, analyzer, problem_data):
        self.__filename = analyzer.get_filename()
        self.__program_dict = analyzer.get_prog_dict()
        self.__sample_outputs = problem_data["outputs"]
        self.__meta = problem_data["metadata"]
        self.__sample_inputs, self.__input_type = problem_data["inputs"], next(iter(self.__meta["input_type"]))
        self.__init_data = problem_data["init_data"]
        self.__num_tests = self.__meta["num_tests"]
        self.__false_flag = True

    def __gen_sub_outputs(self):
        """Private utility method to make hashes from output of provided submission program.

        Returns list of said hashes."""

        sub_outputs = []
        ### need to get underlying OS as syntax of timeout utility differs between linux and mac, and is nonexistent on windows
        platform = sys.platform.lower()
        VERIF_TIMEOUT = "8"
        VERIF_MEMOUT = "1000"
        timeout_cmd = "gtimeout {0}".format(VERIF_TIMEOUT) if platform == "darwin" else "timeout {0} -m {1}".format(VERIF_TIMEOUT, VERIF_MEMOUT) if platform == "linux" or platform == "linux2" else ""
        base_cmd = "{0} python".format(timeout_cmd)
        file_list = []

        ### if input_type is 'file', then iterate over input dict, writing each file to disk as 'file1, file2, filen'
        if self.__input_type == "file":
            for i in range(len(self.__sample_inputs["files"].keys())):
                key = "file_{0}".format(i+1) 
                with open("{0}.py".format(key), 'w') as f:
                    f.write(self.__sample_inputs["files"][key])
                file_list.append("{0}.py".format(key))
            for target_file in file_list:
                output = process_output(base_cmd, self.__filename, input_arg=target_file, init_data=self.__init_data)
                sub_outputs.append(output)
                os.remove(target_file)
        elif self.__input_type == "default":
            if self.__sample_inputs is not None:
                for sample_input in self.__sample_inputs:
                    try:
                        output = process_output(base_cmd, self.__filename, input_arg=json.dumps(sample_input), init_data=self.__init_data)
                    except Exception as e:
                        output = [str(e)]
                    sub_outputs.append(output)
                    ### remove throwaway files after uploaded script has been run on them => if they exist!
            else:
                output = process_output(base_cmd, self.__filename, init_data=self.__init_data)
                sub_outputs.append(output)
        return sub_outputs

    def __detail_inputs(self):
        """Utility function to get info about input type, length, etc.

        Returns tuple of relevant datapoints"""

        if self.__input_type == "file":
            num_tests = len(self.__sample_inputs["files"].keys())
            ### get number of lines in each file (in a list => [len(file1), len(file2), len(fileN)])
            num_keys = len(self.__sample_inputs["files"].keys())
            input_lengths = ["# lines: {0}".format(len(self.__sample_inputs["files"]["file_{0}".format(i+1)].splitlines())) for i in range(num_keys)]
            input_types = ["file" for x in range(num_tests)]
        elif self.__input_type == "default":
            if self.__sample_inputs is not None:
                if self.__meta.get("output_analysis", None) == "one-to-one":
                    num_tests = 1
                    input_lengths = [len(self.__sample_inputs)]
                    input_types = [type(self.__sample_inputs[0]).__name__.lower()]
                else:
                    num_tests = len(self.__sample_inputs)
                    ### note: below only works if i is a list
                    input_lengths = [len(i) for i in self.__sample_inputs]
                    input_types = [type(i).__name__.lower() for i in self.__sample_inputs]
            else:
                num_tests = 1
                input_lengths = ["N/A"]
                input_types = ["No input provided!"]

        return num_tests, input_lengths, input_types
    
    def verify_output(self):
        """Public method for verifying matches between submitted program's output, and sample outputs.

        Returns score as percentage (string) of exact matches (this point may need reviewing)."""
        
        def get_result_stats(count):
            result_dict = {}
            if self.__meta.get("output_analysis", None) == "one-to-one":
                result_dict["one-to-one"] = True
                num_outputs = len(sub_output)
                mismatches = []
                matches = []
                for line_count, (line_sub, line_samp) in enumerate(zip(sub_output, samp_output)):
                    if line_sub != line_samp:
                        mismatches.append((self.__sample_inputs[0][line_count], line_sub, line_samp))
                    else:
                        matches.append((self.__sample_inputs[0][line_count], line_sub, line_samp))
                num_correct = len(matches)
                result_dict["num_correct"] = num_correct
                try:
                    result_dict["success_rate"] = round(num_correct/num_outputs, 4) * 100
                except Exception as e:
                    result_dict["success_rate"] = "File IO"
                num_failures = len(mismatches)
                result_dict["num_failures"] = num_failures
                result_dict["num_tests"] = test_stats[1]
                try:
                    result_dict["failure_rate"] = round((int(num_failures)/num_outputs), 4) * 100
                except Exception as e:
                    result_dict["failure_rate"] = "File IO"
                num_fail_samples = 5 if num_failures > 3 else num_failures
                num_correct_samples = 5 if num_correct > 3 else num_correct
                result_dict["mismatches"] = random.sample(mismatches, num_fail_samples)
                result_dict["matches"] = random.sample(matches, num_correct_samples)
                result_dict["total_mismatches"] = mismatches
                result_dict["total_matches"] = matches

            else:
                result_dict["one-to-one"] = False
                correct = sub_output == samp_output
                result_dict["success"] = correct
                if correct or self.__false_flag:
                    result_dict["input"] = self.__sample_inputs[count]
                    result_dict["user_output"] = sub_output
                    result_dict["reference_output"] = samp_output
                    result_dict["clickable_link"] = True
                    if correct == False and self.__false_flag:
                        self.__false_flag = False
                else:
                    result_dict["clickable_link"] = False
                result_dict["success_rate"] = 1 if correct else 0
                result_dict["failure_rate"] = 1 if not correct else 0
            
            return result_dict

        sample_outputs = self.__sample_outputs
        # print("SAMPOUT =>", sample_outputs)
        sub_outputs = self.__gen_sub_outputs()
        # print("SUBOUT =>", sub_outputs)
        self.__program_dict["scores"] = {}
        scores = self.__program_dict["scores"]
        ### stores input details (cf. __init__)
        test_stats = self.__detail_inputs()
        overall_score = 0
        
        ### loop to compare sub output with sample output in one go, as if they were two columns, one beside the other
        for count, (sub_output, samp_output) in enumerate(zip(sub_outputs, sample_outputs)):
            result_dict = get_result_stats(count)
            status = "success" if result_dict["success_rate"] == 100 or result_dict["success_rate"] == 1 else "failure"
            scores["test_{0}".format(count+1)] = {}
            test = scores["test_{0}".format(count+1)]
            test["status"] = status
            test["input_length"] = test_stats[1][count]
            test["input_type"] = test_stats[2][count]
            test["detailed_stats"] = result_dict
            if result_dict["one-to-one"]:
                overall_score += result_dict["success_rate"] / 100
            elif status == "success":
                overall_score += 1


        ### store score as string
        if self.__meta.get("output_analysis", None) == "one-to-one":
            percentage_score = round(overall_score/len(sample_outputs), 4) * 100
        else:
            percentage_score = round(overall_score/len(self.__sample_inputs), 4) * 100
        scores["overall_score"] = "{0}%".format(percentage_score)
        ### since all pertinent data is written to program dict, we can just return the score here, for code_analysis.views to handle
        return percentage_score