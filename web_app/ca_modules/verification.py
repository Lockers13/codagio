import hashlib 
import subprocess
import sys
import os
import json
from .subprocess_ctrl import run_subprocess_ctrld
from django.utils.translation import gettext_lazy as _

class Verifier:

    """Class for verifying output of submitted program, by checking against sample outputs

    previously generated by the relevant sample reference program during problem upload"""

    def __init__(self, analyzer, paragon):
        self.__filename = analyzer.get_filename()
        self.__program_dict = analyzer.get_prog_dict()
        self.__sample_outputs = json.loads(paragon.outputs)
        self.__meta = json.loads(paragon.metadata)
        self.__sample_inputs, self.__input_type = self.__get_sample_inputs_and_type(json.loads(paragon.inputs))

    def __get_sample_inputs_and_type(self, inputs):

        input_dict = inputs
        input_type = next(iter(input_dict))
        if input_type == "files":
            inputs = input_dict
        elif input_type == "default":
            custom_or_auto = next(iter(input_dict[input_type]))
            inputs = input_dict[input_type][custom_or_auto]
        return inputs, input_type

    def __gen_sub_outputs(self):
        """Private utility method to make hashes from output of provided submission program.

        Returns list of said hashes."""

        sub_outputs = []
        ### need to get underlying OS as syntax of timeout utility differs between linux and mac, and is nonexistent on windows
        platform = sys.platform.lower()
        VERIF_TIMEOUT = "5"
        VERIF_MEMOUT = "500"
        timeout_cmd = "gtimeout {0}".format(VERIF_TIMEOUT) if platform == "darwin" else "timeout {0} -m {1}".format(VERIF_TIMEOUT, VERIF_MEMOUT) if platform == "linux" or platform == "linux2" else ""
        base_cmd = "{0} python".format(timeout_cmd)
        file_list = []

        ### if input_type is 'file', then iterate over input dict, writing each file to disk as 'file1, file2, filen'
        if self.__input_type == "files":
            for k, v in self.__sample_inputs["files"].items():
                with open("{0}.py".format(k), 'w') as f:
                    f.write(v)
                file_list.append("{0}.py".format(k))
        elif self.__input_type == "default":
            pass
        # if we are dealing with files, then we loop over those files, otherwise we loop over the auto generated inputs
        loop_len = len(file_list) if len(file_list) != 0 else len(self.__sample_inputs)
        for i in range(loop_len): 
            ### at this point, the argument we pass from command line is either a script, or an auto-generated piece of json
            cl_param = file_list[i] if self.__input_type == "files" else json.dumps(self.__sample_inputs[i])
            try:
                output = run_subprocess_ctrld(base_cmd, self.__filename, cl_param)
            except Exception as e:
                raise Exception("hhh{0}".format(str(e)))
            ### clean up the returned output of subprocess - '\r' for windows, and 'None' because sometimes python sp.Popen adds this at the end (probably return value)
            cleaned_split_output = output.decode("utf-8").replace('\r', '').replace('None', '').splitlines()
            sub_outputs.append(cleaned_split_output)
            ### remove throwaway files after uploaded script has been run on them => if they exist!
            if len(file_list) > i:
                os.remove(file_list[i])
        return sub_outputs

    def __detail_inputs(self):
        """Utility function to get info about input type, length, etc.

        Returns tuple of relevant datapoints"""

        if self.__input_type == "files":
            num_tests = len(self.__sample_inputs["files"].keys())
            ### one liner to get number of lines in each file (in a list => [len(file1), len(file2), len(fileN)])
            input_lengths = ["# lines: {0}".format(len(v.splitlines())) for k,v in self.__sample_inputs["files"].items()]
            input_types = ["file" for x in range(num_tests)]
        elif self.__input_type == "default":
            num_tests = len(self.__sample_inputs)
            input_lengths = [len(inp) for inp in self.__sample_inputs]
            input_types = [type(inp[0]).__name__.lower() for inp in self.__sample_inputs]
        return num_tests, input_lengths, input_types
    
    def verify_output(self):
        """Public method for verifying matches between submitted program's output, and sample outputs.

        Returns score as percentage (string) of exact matches (this point may need reviewing)."""

        sample_outputs = self.__sample_outputs
        sub_outputs = self.__gen_sub_outputs()
        self.__program_dict["scores"] = {}
        scores = self.__program_dict["scores"]
        ### stores input details (cf. __init__)
        test_stats = self.__detail_inputs()

        overall_score = 0
        ### loop to compare sub output with sample output in one go, as if they were two columns, one beside the other
        for count, (sub_output, samp_output) in enumerate(zip(sub_outputs, sample_outputs)):
            if sub_output == samp_output:
                status = "success"
                overall_score += 1
            else:
                status = "failure"
            scores["test_{0}".format(count+1)] = {}
            test = scores["test_{0}".format(count+1)]
            test["status"] = status
            test["input_length"] = test_stats[1][count]
            test["input_type"] = test_stats[2][count]

        ### store score as string
        percentage_score = round(overall_score/len(sample_outputs), 4) * 100
        scores["overall_score"] = "{0}%".format(percentage_score)
        ### since all pertinent data is written to program dict, we can just return the score here, for code_analysis.views to handle
        return percentage_score